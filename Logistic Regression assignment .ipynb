{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d727549b-5ef9-4172-8992-4d08e53b9fd2",
   "metadata": {},
   "source": [
    " What is Logistic Regression, and how does it differ from Linear Regression.\n",
    "\n",
    "Logistic Regression is a fundamental machine learning algorithm used for classification tasks. Unlike Linear Regression, which predicts continuous values, Logistic Regression estimates the probability that a given instance belongs to a particular class\n",
    "\n",
    "3  Why do we use the Sigmoid function in Logistic Regression.\n",
    "The Sigmoid function is used in Logistic Regression because it transforms any real-valued number into a probability between 0 and 1\n",
    "The output of the Sigmoid function can be directly interpreted as a probability, making it ideal for binary classification\n",
    "The function is continuous and differentiable, which helps in optimization using gradient-based methods\n",
    "- If the probability is greater than 0.5, the instance is classified as 1; otherwise, it is classified as 0\n",
    "\n",
    "\n",
    "4   What is the cost function of Logistic Regression.\n",
    "The cost function in Logistic Regression is called Log Loss or Binary Cross-Entropy Loss. It measures how well the model's predicted probabilities match the actual labels\n",
    "\n",
    "\n",
    "5 What is Regularization in Logistic Regression? Why is it needed.\n",
    "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. It helps the model generalize better to unseen data by discouraging overly complex models\n",
    "\n",
    "\n",
    "6  Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
    "- Use Lasso when you suspect some features are irrelevant and want automatic feature selection.\n",
    "- Use Ridge when all features contribute to the model but need to be controlled to prevent overfitting.\n",
    "- Use Elastic Net when features are correlated, as it combines the benefits of both Lasso and Ridge\n",
    "\n",
    "\n",
    "7 When should we use Elastic Net instead of Lasso or Ridge.\n",
    "Elastic Net is preferred over Lasso or Ridge when features are highly correlated. It combines the strengths of both Lasso (L1 regularization) and Ridge (L2 regularization) to balance feature selection and coefficient shrinkage.\n",
    "\n",
    "\n",
    "8 What is the impact of the regularization parameter (λ) in Logistic Regression\n",
    "The regularization parameter (λ) in Logistic Regression controls the strength of regularization, which helps prevent overfitting and improves generalization.\n",
    "Small λ (Weak Regularization):\n",
    "- The model relies heavily on the training data.\n",
    "- Coefficients can become large, leading to overfitting.\n",
    "- The model may perform well on training data but poorly on unseen data.\n",
    "Large λ (Strong Regularization):\n",
    "- Shrinks coefficients, reducing model complexity.\n",
    "- Prevents overfitting but may lead to underfitting if too strong.\n",
    "\n",
    "\n",
    "9 What are the key assumptions of Logistic Regression\n",
    "Key Assumptions:\n",
    "- Binary or Categorical Dependent Variable: The target variable should be binary (e.g., Yes/No, 0/1) or categorical for multiclass classification.\n",
    "- Independent Observations: Each observation should be independent of the others, meaning no correlation between input variables.\n",
    "- No Multicollinearity: Predictor variables should not be highly correlated with each other, as multicollinearity can distort coefficient estimates.\n",
    "- No Extreme Outliers: Outliers can significantly impact the model’s coefficients, so they should be handled appropriately.\n",
    "\n",
    "\n",
    "10  What are some alternatives to Logistic Regression for classification tasks.\n",
    "Machine Learning-Based Alternatives\n",
    "- Decision Trees – Simple, interpretable models that split data based on feature values.\n",
    "- Random Forest – An ensemble of decision trees that improves accuracy and reduces overfitting.\n",
    "- Support Vector Machines (SVM) – Finds the optimal boundary between classes using hyperplanes.\n",
    "- Naïve Bayes – A probabilistic classifier based on Bayes' theorem, useful for text classification.\n",
    "- K-Nearest Neighbors (KNN) – Classifies based on the majority vote of nearest neighbors.\n",
    "\n",
    "\n",
    "11  What are Classification Evaluation Metrics\n",
    "Classification evaluation metrics help assess the performance of a classification model. Here are some key metrics\n",
    "\n",
    "12  How does class imbalance affect Logistic Regression\n",
    "Effects of Class Imbalance\n",
    "- Biased Predictions – The model tends to classify most instances as the majority class, ignoring the minority class.\n",
    "- Poor Recall for Minority Class – The model struggles to correctly identify minority class instances, leading to high false negatives.\n",
    "- Misleading Accuracy – High accuracy may be deceptive if the model predicts the majority class most of the time.\n",
    "- Skewed Decision Boundary – The model’s decision boundary may not be optimal, making it harder to separate classes effectively.\n",
    "\n",
    "\n",
    "13 C What is Hyperparameter Tuning in Logistic Regression.\n",
    "Key Hyperparameters in Logistic Regression\n",
    "- Regularization Strength (λ or C) – Controls the penalty applied to coefficients to prevent overfitting.\n",
    "- Penalty Type (L1, L2, Elastic Net) – Determines whether Lasso, Ridge, or Elastic Net regularization is used.\n",
    "- Solver Choice – Different optimization algorithms (e.g., liblinear, saga, lbfgs) affect convergence speed and accuracy.\n",
    "- Maximum Iterations (max_iter) – Defines how many iterations the solver runs before stopping.\n",
    "- Class Weights (class_weight) – Adjusts weights for imbalanced datasets to improve minority class predictions.\n",
    "\n",
    "\n",
    "\n",
    "14  What are different solvers in Logistic Regression? Which one should be used.\n",
    "- For large datasets → lbfgs, newton-cg, sag, or saga\n",
    "- For small datasets → liblinear\n",
    "- For Elastic Net regularization → saga\n",
    "\n",
    "\n",
    "15 How is Logistic Regression extended for multiclass classification.\n",
    "One-vs-Rest (OvR)\n",
    "Multinomial Logistic Regression\n",
    "\n",
    "16  What are the advantages and disadvantages of Logistic Regression\n",
    "Advantages --------------\n",
    "- Simple & Interpretable – Easy to understand and interpret compared to complex models.\n",
    "- Efficient & Fast – Works well with small datasets and requires less computational power.\n",
    "- Probabilistic Predictions – Outputs probabilities, making it useful for decision-making.\n",
    "- Handles Categorical & Continuous Variables – Can work with different types of input features.\n",
    "- Less Prone to Overfitting – Regularization techniques (L1, L2) help control complexity.\n",
    "Disadvantages ---------\n",
    "- Assumes Linearity – Requires a linear relationship between independent variables and log-odds.\n",
    "- Sensitive to Outliers – Extreme values can distort predictions.\n",
    "- Struggles with Complex Relationships – Cannot model highly non-linear patterns well.\n",
    "- Requires Feature Engineering – Needs well-prepared input data for optimal performance.\n",
    "- Not Ideal for Large Feature Sets – Can overfit when the number of features is much larger than observations.\n",
    "\n",
    "\n",
    "17 What are some use cases of Logistic Regression\n",
    "1  Medical Diagnosis\n",
    "- Predicting the likelihood of diseases (e.g., diabetes, heart disease).\n",
    "- Identifying risk factors based on patient data.\n",
    "2. Fraud Detection\n",
    "- Detecting fraudulent transactions in banking and e-commerce.\n",
    "- Identifying suspicious activities in cybersecurity.\n",
    "3. Marketing & Customer Analytics\n",
    "- Predicting customer churn (whether a customer will leave a service).\n",
    "- Classifying potential leads as high or low conversion prospects.\n",
    "4. Spam Detection\n",
    "- Filtering spam emails based on text patterns.\n",
    "- Identifying phishing attempts.\n",
    "5. Credit Scoring & Risk Asses\n",
    "\n",
    "\n",
    "18 How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification\n",
    "One-vs-Rest (OvR)\n",
    "- Trains multiple binary classifiers, one for each class.\n",
    "- Each classifier predicts whether an instance belongs to a specific class or not.\n",
    "- The class with the highest probability is chosen.\n",
    "Softmax Regression\n",
    "- Uses the Softmax function instead of the Sigmoid function.\n",
    "- Computes probabilities for all classes simultaneously.\n",
    "- The class with the highest probability is selected.\n",
    "\n",
    "\n",
    "19 How do we interpret coefficients in Logistic Regression?\n",
    "- Log-Odds Representation\n",
    "- Each coefficient represents the change in log-odds of the dependent variable for a one-unit increase in the predictor variable.\n",
    "- If a coefficient is positive, it increases the probability of the event occurring.\n",
    "- If a coefficient is negative, it decreases the probability of the event occurring.\n",
    "Interpreting Categorical Variables\n",
    "- For binary categorical predictors (e.g., Male vs. Female), the coefficient represents the difference in log-odds between the two categories.\n",
    "- For multi-category predictors, one category is chosen as the reference, and coefficients represent changes relative to that reference.\n",
    "Impact of Large Coefficients\n",
    "Large coefficients indicate a strong effect of the predictor on the outcome.\n",
    "Small coefficients suggest a weak or negligible effect.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34bf37b-d1b1-4acd-9014-011d2827eb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#  Load a dataset, split it, apply Logistic Regression, and print accuracy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  \n",
    "\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=200)  # Increase iterations to prevent convergence warnings\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efa8b5a9-b2a4-4ca7-9119-0328c16fe56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Regularization Model Accuracy: 1.0\n",
      "Model Coefficients: [[-0.39339961  0.96258869 -2.37510705 -0.99874611]\n",
      " [ 0.5084024  -0.25486663 -0.21301372 -0.77575531]\n",
      " [-0.11500279 -0.70772206  2.58812078  1.77450141]]\n"
     ]
    }
   ],
   "source": [
    "#   Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
    "# and print the model accuracy\n",
    "\n",
    "\n",
    "model = LogisticRegression(penalty='l2') \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"L2 Regularization Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Model Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48b86fd-ad42-4ab4-98a6-dcc7dca3403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Model Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
    "# LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC\n",
    "\n",
    "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)  # ElasticNet requires 'saga' solver\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Elastic Net Model Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7da170a-72fa-4b26-9b7a-2bd1b48ce684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvR Model Accuracy: 0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')C\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr', solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"OvR Model Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1447428-4a3b-4838-b6b6-38b523c70ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best Accuracy: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train a Logistic Regression model for multiclass classification using\n",
    "# multi_class='ovr'\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5297120-8f6f-4587-937a-40bd2837b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy using Stratified K-Fold: 0.9733333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracyC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Average Accuracy using Stratified K-Fold:\", np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98b01880-89eb-4fa8-b11b-acfe86dcb70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Dataset Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracyC\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  \n",
    "\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"CSV Dataset Model Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbae5203-9b01-4805-8b7d-9e1c33dd32ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-One (OvO) Logistic Regression Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# M Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"One-vs-One (OvO) Logistic Regression Model Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ea613dd-1e68-46e2-980b-1d39df985daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "#  Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-ScoreM\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d323cc36-c753-4017-8b3b-110f74ce8c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.852\n",
      "Precision: 0.40271493212669685\n",
      "Recall: 0.8476190476190476\n",
      "F1-Score: 0.5460122699386503\n"
     ]
    }
   ],
   "source": [
    "#  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performanceM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=5000, random_state=42)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a000fd8-6933-483e-ab09-a801b0704ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy WITHOUT scaling: 1.0\n",
      "Accuracy WITH scaling: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scalingM\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_no_scaling = LogisticRegression(max_iter=200)\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model_scaled = LogisticRegression(max_iter=200)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "\n",
    "print(\"Accuracy WITHOUT scaling:\", accuracy_no_scaling)\n",
    "print(\"Accuracy WITH scaling:\", accuracy_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f4e15ce-2ed9-4a30-ba1f-6e61ca3b8e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.9289692404030436\n"
     ]
    }
   ],
   "source": [
    "#  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scoreM \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X, y = make_classification(n_classes=2, weights=[0.7, 0.3], n_samples=5000, random_state=42)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:, 1] \n",
    "\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "604669f5-894a-4478-9aae-e81c79496ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy with C=0.5: 1.0\n"
     ]
    }
   ],
   "source": [
    "# M Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
    "# accuracy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = LogisticRegression(C=0.5, max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Logistic Regression Model Accuracy with C=0.5:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abbba297-a4bd-4463-b49e-6578f46862b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: liblinear | Accuracy: 1.0000\n",
      "Solver: saga | Accuracy: 1.0000\n",
      "Solver: lbfgs | Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
    "# their accuracy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "\n",
    "\n",
    "for solver in solvers:\n",
    "    model = LogisticRegression(solver=solver, max_iter=200)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "   \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Solver: {solver} | Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ce69a72-c6ed-4c5a-926b-2166e5fad871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.886\n",
      "Matthews Correlation Coefficient (MCC): 0.6244685699066264\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
    "# Correlation Coefficient (MCC)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X, y = make_classification(n_classes=2, weights=[0.8, 0.2], n_samples=5000, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "580ff68d-8bd3-49c7-a14c-4fa41aab0aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C: 0.46415888336127775\n",
      "Best Cross-Validation Accuracy: 0.9583\n",
      "Test Accuracy with Optimal C: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
    "# cross-validation\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "C_values = np.logspace(-3, 3, 10)  \n",
    "\n",
    "best_score = 0\n",
    "best_C = None\n",
    "\n",
    "\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(C=C, max_iter=200)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')  # 5-fold CV\n",
    "    avg_score = scores.mean()\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_C = C\n",
    "\n",
    "print(f\"Optimal C: {best_C}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "\n",
    "final_model = LogisticRegression(C=best_C, max_iter=200)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "test_accuracy = final_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with Optimal C: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8baac81-744f-4085-a3ca-1f79d399307a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
